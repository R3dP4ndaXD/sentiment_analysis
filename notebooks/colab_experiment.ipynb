{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0def77f",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670a7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88055ae2",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (for persistent storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8230e718",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-890282644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Create project directory in Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create project directory in Drive\n",
    "    os.makedirs('/content/drive/MyDrive/ML_Sentiment_Analysis/checkpoints', exist_ok=True)\n",
    "    os.makedirs('/content/drive/MyDrive/ML_Sentiment_Analysis/results', exist_ok=True)\n",
    "    os.makedirs('/content/drive/MyDrive/ML_Sentiment_Analysis/plots', exist_ok=True)\n",
    "    \n",
    "    BASE_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis\"\n",
    "    print(\"‚úÖ Google Drive mounted!\")\n",
    "else:\n",
    "    # Running locally - use project directory\n",
    "    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\".\")))\n",
    "    if not os.path.exists(os.path.join(BASE_DIR, \"src\")):\n",
    "        # Fallback: use current working directory's parent\n",
    "        BASE_DIR = os.path.dirname(os.getcwd())\n",
    "    \n",
    "    os.makedirs(os.path.join(BASE_DIR, 'checkpoints'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'results'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'plots'), exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Running locally. Base directory: {BASE_DIR}\")\n",
    "\n",
    "print(f\"IN_COLAB: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e4e03",
   "metadata": {},
   "source": [
    "## 3. Clone/Upload Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Option A: Clone from GitHub (uncomment and replace with your repo)\n",
    "    # !git clone https://github.com/yourusername/sentiment_analysis.git\n",
    "    # %cd sentiment_analysis\n",
    "\n",
    "    # Option B: Upload from local (run this cell, then upload the zip)\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Upload your project as a .zip file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Extract the uploaded zip\n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.zip'):\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content/')\n",
    "            print(f\"‚úÖ Extracted {filename}\")\n",
    "            # Find the extracted directory\n",
    "            extracted_dirs = [d for d in os.listdir('/content/') if os.path.isdir(f'/content/{d}') and d != 'drive' and d != 'sample_data']\n",
    "            if extracted_dirs:\n",
    "                os.chdir(f'/content/{extracted_dirs[0]}')\n",
    "                print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    # Running locally - change to project root\n",
    "    project_root = os.path.dirname(os.path.dirname(os.path.abspath(\".\")))\n",
    "    if os.path.exists(os.path.join(project_root, \"src\")):\n",
    "        os.chdir(project_root)\n",
    "    else:\n",
    "        # Try parent of current directory\n",
    "        project_root = os.path.dirname(os.getcwd())\n",
    "        if os.path.exists(os.path.join(project_root, \"src\")):\n",
    "            os.chdir(project_root)\n",
    "    \n",
    "    print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4927087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
      "drwxr-xr-x 1 root root 4096 Jan  3 16:50 ..\n",
      "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n",
      "\n",
      "üìÇ Source directory:\n",
      "ls: cannot access 'src/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Verify project structure\n",
    "!ls -la\n",
    "print(\"\\nüìÇ Source directory:\")\n",
    "!ls -la src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e294d6",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from requirements.txt if exists\n",
    "!pip install -q -r requirements.txt 2>/dev/null || echo \"No requirements.txt found\"\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q torch pandas scikit-learn matplotlib seaborn spacy\n",
    "\n",
    "# Download Romanian spaCy model\n",
    "!python -m spacy download ro_core_news_sm -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94a886",
   "metadata": {},
   "source": [
    "## 5. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Download ro_sent dataset\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/test.csv\"\n",
    "\n",
    "!wget -q -O data/raw/train.csv \"{TRAIN_URL}\" 2>/dev/null || echo \"Downloading train.csv...\"\n",
    "!wget -q -O data/raw/test.csv \"{TEST_URL}\" 2>/dev/null || echo \"Downloading test.csv...\"\n",
    "\n",
    "# Check if download succeeded, if not try alternative method\n",
    "if not os.path.exists('data/raw/train.csv') or os.path.getsize('data/raw/train.csv') < 1000:\n",
    "    print(\"Trying alternative download method...\")\n",
    "    # Use datasets library as fallback\n",
    "    !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"dumitrescustefan/ro_sent\")\n",
    "    dataset['train'].to_pandas().to_csv('data/raw/train.csv', index=False)\n",
    "    dataset['test'].to_pandas().to_csv('data/raw/test.csv', index=False)\n",
    "\n",
    "# Verify download\n",
    "train_df = pd.read_csv('data/raw/train.csv')\n",
    "test_df = pd.read_csv('data/raw/test.csv')\n",
    "print(f\"‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8464fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train into train/val (90/10)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Save processed splits\n",
    "train_data.to_csv('data/processed/train.csv', index=False)\n",
    "val_data.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245029a",
   "metadata": {},
   "source": [
    "## 6. Run Experiments\n",
    "\n",
    "Configure and run your training experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for persistent storage - dynamically set based on environment\n",
    "if IN_COLAB:\n",
    "    CHECKPOINT_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/checkpoints\"\n",
    "    RESULTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/results\"\n",
    "    PLOTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/plots\"\n",
    "else:\n",
    "    CHECKPOINT_DIR = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "    RESULTS_DIR = os.path.join(os.getcwd(), \"results\")\n",
    "    PLOTS_DIR = os.path.join(os.getcwd(), \"plots\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"Plots dir: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741b4cf",
   "metadata": {},
   "source": [
    "### Experiment 1: LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a555918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"src.run_experiment\",\n",
    "    \"--model\", \"lstm\",\n",
    "    \"--hidden_dim\", \"256\",\n",
    "    \"--num_layers\", \"2\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--lr\", \"0.001\",\n",
    "    \"--device\", \"cuda\" if IN_COLAB else \"cpu\",\n",
    "    \"--experiment_name\", \"lstm_baseline\",\n",
    "    \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
    "    \"--results_dir\", RESULTS_DIR,\n",
    "    \"--plots_dir\", PLOTS_DIR\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c5f59",
   "metadata": {},
   "source": [
    "### Experiment 2: BiLSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"src.run_experiment\",\n",
    "    \"--model\", \"bilstm_attention\",\n",
    "    \"--hidden_dim\", \"256\",\n",
    "    \"--num_layers\", \"2\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--lr\", \"0.001\",\n",
    "    \"--device\", \"cuda\" if IN_COLAB else \"cpu\",\n",
    "    \"--experiment_name\", \"bilstm_attention\",\n",
    "    \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
    "    \"--results_dir\", RESULTS_DIR,\n",
    "    \"--plots_dir\", PLOTS_DIR\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655552e",
   "metadata": {},
   "source": [
    "### Experiment 3: LSTM with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e79436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"src.run_experiment\",\n",
    "    \"--model\", \"lstm\",\n",
    "    \"--hidden_dim\", \"256\",\n",
    "    \"--num_layers\", \"2\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--lr\", \"0.001\",\n",
    "    \"--augment\", \"random_swap\",\n",
    "    \"--aug_prob\", \"0.1\",\n",
    "    \"--device\", \"cuda\" if IN_COLAB else \"cpu\",\n",
    "    \"--experiment_name\", \"lstm_augmented\",\n",
    "    \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
    "    \"--results_dir\", RESULTS_DIR,\n",
    "    \"--plots_dir\", PLOTS_DIR\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbcdf0c",
   "metadata": {},
   "source": [
    "### Experiment 4: Simple RNN (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"src.run_experiment\",\n",
    "    \"--model\", \"simple_rnn\",\n",
    "    \"--hidden_dim\", \"256\",\n",
    "    \"--num_layers\", \"2\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--lr\", \"0.001\",\n",
    "    \"--device\", \"cuda\" if IN_COLAB else \"cpu\",\n",
    "    \"--experiment_name\", \"simple_rnn_baseline\",\n",
    "    \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
    "    \"--results_dir\", RESULTS_DIR,\n",
    "    \"--plots_dir\", PLOTS_DIR\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aa1ea",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all experiment summaries\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "summaries = []\n",
    "\n",
    "if results_dir.exists():\n",
    "    for exp_dir in results_dir.iterdir():\n",
    "        summary_file = exp_dir / 'summary.json'\n",
    "        if summary_file.exists():\n",
    "            with open(summary_file) as f:\n",
    "                summary = json.load(f)\n",
    "                summary['experiment'] = exp_dir.name\n",
    "                summaries.append(summary)\n",
    "\n",
    "if summaries:\n",
    "    df = pd.DataFrame(summaries)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    display(df[['experiment', 'model', 'best_val_f1', 'best_val_acc', 'epochs_trained']].sort_values('best_val_f1', ascending=False))\n",
    "else:\n",
    "    print(\"No results found yet. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2315c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots from experiments\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "plots_dir = Path(PLOTS_DIR)\n",
    "if plots_dir.exists():\n",
    "    for exp_dir in plots_dir.iterdir():\n",
    "        print(f\"\\nüìä {exp_dir.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        for plot in exp_dir.glob('*.png'):\n",
    "            print(f\"\\n{plot.name}:\")\n",
    "            display(Image(filename=str(plot), width=600))\n",
    "else:\n",
    "    print(f\"Plots directory not found: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a8e6",
   "metadata": {},
   "source": [
    "## 8. Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ae040",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import subprocess\n",
    "    # Zip and download all results\n",
    "    subprocess.run([\"zip\", \"-r\", \"/content/experiment_results.zip\", RESULTS_DIR, PLOTS_DIR, CHECKPOINT_DIR])\n",
    "    \n",
    "    from google.colab import files\n",
    "    files.download('/content/experiment_results.zip')\n",
    "    print(\"‚úÖ Results downloaded!\")\n",
    "else:\n",
    "    print(f\"Results are saved locally at:\")\n",
    "    print(f\"  - Results: {RESULTS_DIR}\")\n",
    "    print(f\"  - Plots: {PLOTS_DIR}\")\n",
    "    print(f\"  - Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04d700",
   "metadata": {},
   "source": [
    "## üîß Custom Experiment Runner\n",
    "\n",
    "Use this cell to run custom experiments with your own parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Custom experiment configuration\n",
    "CONFIG = {\n",
    "    'model': 'lstm',              # simple_rnn, stacked_rnn, lstm, bilstm_attention, lstm_batchnorm, stacked_lstm\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3,\n",
    "    'bidirectional': False,       # Only for lstm, lstm_batchnorm, stacked_lstm\n",
    "    'pooling': 'last',            # last, max, mean\n",
    "    \n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': 'adamw',         # adam, adamw, sgd\n",
    "    'scheduler': 'plateau',       # plateau, cosine, none\n",
    "    'weight_decay': 1e-5,\n",
    "    'gradient_clip': 1.0,\n",
    "    \n",
    "    'augment': None,              # random_swap, random_delete, random_insert, random_shuffle, eda, None\n",
    "    'aug_prob': 0.1,\n",
    "    \n",
    "    'early_stopping': 5,\n",
    "    'experiment_name': 'custom_experiment',\n",
    "}\n",
    "\n",
    "# Build command\n",
    "cmd = [\"python\", \"-m\", \"src.run_experiment\"]\n",
    "for key, value in CONFIG.items():\n",
    "    if value is not None and value is not False:\n",
    "        if isinstance(value, bool):\n",
    "            cmd.append(f\"--{key}\")\n",
    "        else:\n",
    "            cmd.extend([f\"--{key}\", str(value)])\n",
    "\n",
    "cmd.extend([\"--device\", \"cuda\" if IN_COLAB else \"cpu\"])\n",
    "cmd.extend([\"--checkpoint_dir\", CHECKPOINT_DIR])\n",
    "cmd.extend([\"--results_dir\", RESULTS_DIR])\n",
    "cmd.extend([\"--plots_dir\", PLOTS_DIR])\n",
    "\n",
    "print(\"Running command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "subprocess.run(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
