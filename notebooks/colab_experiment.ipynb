{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0def77f",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670a7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"âš ï¸ No GPU detected! Go to Runtime â†’ Change runtime type â†’ GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88055ae2",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (for persistent storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: True\n",
      "ðŸ“ Using /content for storage\n",
      "âš ï¸ Remember to download results before session ends!\n",
      "BASE_DIR: /content\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "BASE_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis\"\n",
    "os.makedirs(os.path.join(BASE_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'results'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'plots'), exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive mounted!\")\n",
    "print(f\"ðŸ“ Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e4e03",
   "metadata": {},
   "source": [
    "## 3. Clone/Upload Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Cloning from https://github.com/R3dP4ndaXD/sentiment_analysis...\n",
      "âœ… Downloaded and extracted to: /content/sentiment_analysis\n",
      "\n",
      "ðŸ“ Working directory: /content/sentiment_analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==================== CONFIGURE YOUR REPO HERE ====================\n",
    "GITHUB_REPO = \"https://github.com/R3dP4ndaXD/sentiment_analysis.git\"\n",
    "# ==================================================================\n",
    "\n",
    "repo_name = \"sentiment_analysis\"\n",
    "target_dir = f'/content/{repo_name}'\n",
    "\n",
    "# Ensure we are in a stable directory before attempting to clone\n",
    "# This helps avoid issues if the previous working directory was deleted\n",
    "os.chdir('/content')\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists(target_dir):\n",
    "    !rm -rf {target_dir}\n",
    "\n",
    "# Clone from GitHub\n",
    "!git clone {GITHUB_REPO} {target_dir}\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(target_dir)\n",
    "print(f\"âœ… Cloned {GITHUB_REPO}\")\n",
    "print(f\"ðŸ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4927087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1172\n",
      "drwxr-xr-x  2 r3dp4nda r3dp4nda    4096 Jan  6 12:36 .\n",
      "drwxr-xr-x 11 r3dp4nda r3dp4nda    4096 Jan  7 15:20 ..\n",
      "-rw-r--r--  1 r3dp4nda r3dp4nda   21150 Jan 10 12:30 colab_experiment.ipynb\n",
      "-rw-r--r--  1 r3dp4nda r3dp4nda 1150657 Jan  6 10:05 data_exploration.ipynb\n",
      "-rw-r--r--  1 r3dp4nda r3dp4nda   16172 Jan 10 11:35 offline_augmentation_colab.ipynb\n",
      "\n",
      "ðŸ“‚ Source directory:\n",
      "ls: cannot access 'src/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Verify project structure\n",
    "!ls -la\n",
    "print(\"\\nðŸ“‚ Source directory:\")\n",
    "!ls -la src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e294d6",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab59f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ro_core_news_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "âœ… Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install from requirements.txt if exists\n",
    "!pip install -q -r requirements.txt 2>/dev/null || echo \"No requirements.txt found\"\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q torch pandas scikit-learn matplotlib seaborn spacy\n",
    "\n",
    "# Download Romanian spaCy model\n",
    "!python -m spacy download ro_core_news_sm -q\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94a886",
   "metadata": {},
   "source": [
    "## 5. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cb378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train samples: 17941\n",
      "âœ… Test samples: 11005\n",
      "\n",
      "Columns: ['index', 'text', 'label']\n",
      "\n",
      "Label distribution (train):\n",
      "label\n",
      "1    11094\n",
      "0     6847\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Download ro_sent dataset\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/test.csv\"\n",
    "\n",
    "!wget -q -O data/raw/train.csv \"{TRAIN_URL}\" 2>/dev/null || echo \"Downloading train.csv...\"\n",
    "!wget -q -O data/raw/test.csv \"{TEST_URL}\" 2>/dev/null || echo \"Downloading test.csv...\"\n",
    "\n",
    "# Check if download succeeded, if not try alternative method\n",
    "if not os.path.exists('data/raw/train.csv') or os.path.getsize('data/raw/train.csv') < 1000:\n",
    "    print(\"Trying alternative download method...\")\n",
    "    # Use datasets library as fallback\n",
    "    !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"dumitrescustefan/ro_sent\")\n",
    "    dataset['train'].to_pandas().to_csv('data/raw/train.csv', index=False)\n",
    "    dataset['test'].to_pandas().to_csv('data/raw/test.csv', index=False)\n",
    "\n",
    "# Verify download\n",
    "train_df = pd.read_csv('data/raw/train.csv')\n",
    "test_df = pd.read_csv('data/raw/test.csv')\n",
    "print(f\"âœ… Train samples: {len(train_df)}\")\n",
    "print(f\"âœ… Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8464fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train: 16146 | Val: 1795 | Test: 11005\n"
     ]
    }
   ],
   "source": [
    "# Create train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train into train/val (85/15)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Save processed splits\n",
    "DATA_DIR = \"data/processed\"\n",
    "train_data.to_csv('data/processed/train.csv', index=False)\n",
    "val_data.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245029a",
   "metadata": {},
   "source": [
    "## 6. Run Experiments\n",
    "\n",
    "Configure and run your training experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b1566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint dir: /content/checkpoints\n",
      "Results dir: /content/results\n",
      "Plots dir: /content/plots\n"
     ]
    }
   ],
   "source": [
    "# Paths for persistent storage on Google Drive\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/checkpoints\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/results\"\n",
    "PLOTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/plots\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"Plots dir: {PLOTS_DIR}\")\n",
    "\n",
    "# ==================== EMBEDDINGS CONFIG ====================\n",
    "# Download Romanian fastText embeddings (run once - ~4.5GB)\n",
    "# !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ro.300.bin.gz\n",
    "# !gunzip cc.ro.300.bin.gz\n",
    "# !mv cc.ro.300.bin /content/drive/MyDrive/ML_Sentiment_Analysis/\n",
    "\n",
    "FASTTEXT_PATH = \"/content/drive/MyDrive/ML_Sentiment_Analysis/cc.ro.300.bin\"\n",
    "FREEZE_EMBEDDINGS = False\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Paths for persistent storage on Google Drive\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/checkpoints\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/results\"\n",
    "PLOTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/plots\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"Plots dir: {PLOTS_DIR}\")\n",
    "\n",
    "# ==================== EMBEDDINGS CONFIG ====================\n",
    "# Download Romanian fastText embeddings (run once - ~4.5GB)\n",
    "# !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ro.300.bin.gz\n",
    "# !gunzip cc.ro.300.bin.gz\n",
    "# !mv cc.ro.300.bin /content/drive/MyDrive/ML_Sentiment_Analysis/\n",
    "\n",
    "FASTTEXT_PATH = \"/content/drive/MyDrive/ML_Sentiment_Analysis/cc.ro.300.bin\"\n",
    "FREEZE_EMBEDDINGS = False\n",
    "# ============================================================\n",
    "\n",
    "# ==================== CONFIG HELPER ====================\n",
    "def run_experiment(base_params, specific_params):\n",
    "    \"\"\"Helper to run experiments with specific configurations.\"\"\"\n",
    "    config = base_params.copy()\n",
    "    config.update(specific_params)\n",
    "    \n",
    "    # Auto-generate experiment name if not set\n",
    "    if 'experiment_name' not in config:\n",
    "        aug_type = config.get('augment', 'none')\n",
    "        aug_suffix = \"_noaug\" if aug_type == 'none' else f\"_{aug_type}\"\n",
    "        if aug_type != 'none':\n",
    "            aug_prob = config.get('aug_prob', 0.5)\n",
    "            aug_suffix += f\"_p{aug_prob}\"\n",
    "\n",
    "        model = config['model']\n",
    "        if 'bi' not in config.get('model') and config.get('bidirectional'):\n",
    "            model += '_bi'\n",
    "        \n",
    "        expand_factor = config.get('expand_factor', 1.0)\n",
    "        expand_suffix = \"\" if expand_factor == 1.0 else f\"_exp{expand_factor}\"\n",
    "        \n",
    "        hidden_dim = config.get('hidden_dim', 256)\n",
    "        hidden_dim_suffix = \"\" if hidden_dim == 256 else f\"_hd{hidden_dim}\"\n",
    "\n",
    "        num_layers = config.get('num_layers', 2)\n",
    "        num_layers_suffix = \"\" if num_layers == 2 else f\"_nl{num_layers}\"\n",
    "\n",
    "        config['experiment_name'] = (\n",
    "            f\"{model}\"\n",
    "            f\"{hidden_dim_suffix}\"\n",
    "            f\"{expand_suffix}\"\n",
    "            f\"{num_layers_suffix}\"\n",
    "            f\"{aug_suffix}\"\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸš€ Starting Experiment: {config['experiment_name']}\")\n",
    "    \n",
    "    # Build command string\n",
    "    cmd_parts = [\"python -m src.run_experiment\"]\n",
    "    for k, v in config.items():\n",
    "        if isinstance(v, bool):\n",
    "            if v: cmd_parts.append(f\"--{k}\")\n",
    "        else:\n",
    "            cmd_parts.append(f\"--{k} {v}\")\n",
    "            \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Executing: {cmd}\\n\")\n",
    "    \n",
    "    # Execute using IPython magic for real-time output\n",
    "    get_ipython().system(cmd)\n",
    "\n",
    "# Base Configuration Map\n",
    "BASE_CONFIG = {\n",
    "    'model': 'lstm',\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.5,\n",
    "    'pooling': 'max',\n",
    "    'epochs': 30,\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 1e-5,\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'plateau',\n",
    "    'gradient_clip': 1.0,\n",
    "    'max_seq_len': 160,\n",
    "    'min_freq': 2,\n",
    "    'max_vocab_size': 50000,\n",
    "    'pretrained_embeddings': FASTTEXT_PATH,\n",
    "    'early_stopping': 5,\n",
    "    'checkpoint_metric': 'val_f1',\n",
    "    'device': DEVICE,\n",
    "    'data_dir': DATA_DIR,\n",
    "    'checkpoint_dir': CHECKPOINT_DIR,\n",
    "    'plots_dir': PLOTS_DIR,\n",
    "    'results_dir': RESULTS_DIR,\n",
    "    'remove_stopwords': False,\n",
    "    'weighted_sampler': True,\n",
    "    'balance_classes': False,\n",
    "    'expand_factor': 1.0,\n",
    "    'bidirectional': True,\n",
    "    'num_workers': 2,\n",
    "    'freeze_embeddings': True\n",
    "}\n",
    "    #--resume {PATH}\n",
    "    #--evaluate_only /\n",
    "    #--checkpoint {PATH} \\\n",
    "    #--verbose \\\n",
    "    #--no_plots \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91880724",
   "metadata": {},
   "source": [
    "No aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Augmentation Experiment\n",
    "no_aug_params = {\n",
    "    'augment': 'none',\n",
    "    'num_workers': 2,\n",
    "}\n",
    "\n",
    "run_experiment(BASE_CONFIG, no_aug_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3011cb",
   "metadata": {},
   "source": [
    "Aug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Experiment\n",
    "BALANCED_DATA_DIR = '/content/drive/MyDrive/ML_Sentiment_Analysis/data/balanced/'\n",
    "\n",
    "aug_params = {\n",
    "    'augment': 'eda_plus',\n",
    "    'aug_prob': 0.1,\n",
    "    'aug_mode': 'one_of',\n",
    "    'data_dir': BALANCED_DATA_DIR,\n",
    "    'num_workers': 0  # Lower workers for complex augmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aa1ea",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all experiment summaries\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "summaries = []\n",
    "\n",
    "if results_dir.exists():\n",
    "    for exp_dir in results_dir.iterdir():\n",
    "        if exp_dir.is_dir():\n",
    "            summary_file = exp_dir / 'summary.json'\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file) as f:\n",
    "                    summary = json.load(f)\n",
    "                    summary['experiment'] = exp_dir.name\n",
    "                    summaries.append(summary)\n",
    "\n",
    "if summaries:\n",
    "    df = pd.DataFrame(summaries)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    display(df[['experiment', 'model', 'best_val_f1', 'best_val_acc', 'epochs_trained']].sort_values('best_val_f1', ascending=False))\n",
    "else:\n",
    "    print(\"No results found yet. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2315c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots from experiments\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "plots_dir = Path(PLOTS_DIR)\n",
    "if plots_dir.exists():\n",
    "    for exp_dir in sorted(plots_dir.iterdir()):\n",
    "        if exp_dir.is_dir():\n",
    "            print(f\"\\nðŸ“Š {exp_dir.name}\")\n",
    "            print(\"-\" * 40)\n",
    "            for plot in sorted(exp_dir.glob('*.png')):\n",
    "                print(f\"\\n{plot.name}:\")\n",
    "                display(Image(filename=str(plot), width=600))\n",
    "else:\n",
    "    print(f\"Plots directory not found: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a8e6",
   "metadata": {},
   "source": [
    "## 8. Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ae040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download all results\n",
    "!zip -r /content/experiment_results.zip {RESULTS_DIR} {PLOTS_DIR}\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/experiment_results.zip')\n",
    "print(\"âœ… Results downloaded!\")\n",
    "\n",
    "# Note: Results are also saved to Google Drive at:\n",
    "print(f\"\\nðŸ“ Results persist in Google Drive:\")\n",
    "print(f\"   {RESULTS_DIR}\")\n",
    "print(f\"   {PLOTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
