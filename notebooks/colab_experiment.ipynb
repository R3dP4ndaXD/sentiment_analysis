{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0def77f",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88055ae2",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (for persistent storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: True\n",
      "üìÅ Using /content for storage\n",
      "‚ö†Ô∏è Remember to download results before session ends!\n",
      "BASE_DIR: /content\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "BASE_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis\"\n",
    "os.makedirs(os.path.join(BASE_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'results'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'plots'), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted!\")\n",
    "print(f\"üìÅ Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e4e03",
   "metadata": {},
   "source": [
    "## 3. Clone/Upload Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cloning from https://github.com/R3dP4ndaXD/sentiment_analysis...\n",
      "‚úÖ Downloaded and extracted to: /content/sentiment_analysis\n",
      "\n",
      "üìÅ Working directory: /content/sentiment_analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==================== CONFIGURE YOUR REPO HERE ====================\n",
    "GITHUB_REPO = \"https://github.com/R3dP4ndaXD/sentiment_analysis.git\"\n",
    "# ==================================================================\n",
    "\n",
    "repo_name = \"sentiment_analysis\"\n",
    "target_dir = f'/content/{repo_name}'\n",
    "\n",
    "# Ensure we are in a stable directory before attempting to clone\n",
    "# This helps avoid issues if the previous working directory was deleted\n",
    "os.chdir('/content')\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists(target_dir):\n",
    "    !rm -rf {target_dir}\n",
    "\n",
    "# Clone from GitHub\n",
    "!git clone {GITHUB_REPO} {target_dir}\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(target_dir)\n",
    "print(f\"‚úÖ Cloned {GITHUB_REPO}\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4927087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 7 root root 4096 Jan  3 21:05 .\n",
      "drwxr-xr-x 1 root root 4096 Jan  3 21:05 ..\n",
      "-rw-r--r-- 1 root root 6714 Jan  3 21:05 cerinta.md\n",
      "drwxr-xr-x 3 root root 4096 Jan  3 21:05 data\n",
      "drwxr-xr-x 2 root root 4096 Jan  3 21:05 .github\n",
      "-rw-r--r-- 1 root root  593 Jan  3 21:05 .gitignore\n",
      "drwxr-xr-x 2 root root 4096 Jan  3 21:05 notebooks\n",
      "-rw-r--r-- 1 root root  153 Jan  3 21:05 requirements.txt\n",
      "drwxr-xr-x 9 root root 4096 Jan  3 21:05 results\n",
      "drwxr-xr-x 8 root root 4096 Jan  3 21:05 src\n",
      "\n",
      "üìÇ Source directory:\n",
      "total 56\n",
      "drwxr-xr-x 8 root root  4096 Jan  3 21:05 .\n",
      "drwxr-xr-x 7 root root  4096 Jan  3 21:05 ..\n",
      "-rw-r--r-- 1 root root  3687 Jan  3 21:05 config.py\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 data\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 embeddings\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 evaluate\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 models\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 preprocessing\n",
      "-rw-r--r-- 1 root root 19259 Jan  3 21:05 run_experiment.py\n",
      "drwxr-xr-x 2 root root  4096 Jan  3 21:05 train\n"
     ]
    }
   ],
   "source": [
    "# Verify project structure\n",
    "!ls -la\n",
    "print(\"\\nüìÇ Source directory:\")\n",
    "!ls -la src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e294d6",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab59f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ro_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "‚úÖ Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install from requirements.txt if exists\n",
    "!pip install -q -r requirements.txt 2>/dev/null || echo \"No requirements.txt found\"\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q torch pandas scikit-learn matplotlib seaborn spacy\n",
    "\n",
    "# Download Romanian spaCy model\n",
    "!python -m spacy download ro_core_news_sm -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94a886",
   "metadata": {},
   "source": [
    "## 5. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cb378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train samples: 17941\n",
      "‚úÖ Test samples: 11005\n",
      "\n",
      "Columns: ['index', 'text', 'label']\n",
      "\n",
      "Label distribution (train):\n",
      "label\n",
      "1    11094\n",
      "0     6847\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Download ro_sent dataset\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/test.csv\"\n",
    "\n",
    "!wget -q -O data/raw/train.csv \"{TRAIN_URL}\" 2>/dev/null || echo \"Downloading train.csv...\"\n",
    "!wget -q -O data/raw/test.csv \"{TEST_URL}\" 2>/dev/null || echo \"Downloading test.csv...\"\n",
    "\n",
    "# Check if download succeeded, if not try alternative method\n",
    "if not os.path.exists('data/raw/train.csv') or os.path.getsize('data/raw/train.csv') < 1000:\n",
    "    print(\"Trying alternative download method...\")\n",
    "    # Use datasets library as fallback\n",
    "    !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"dumitrescustefan/ro_sent\")\n",
    "    dataset['train'].to_pandas().to_csv('data/raw/train.csv', index=False)\n",
    "    dataset['test'].to_pandas().to_csv('data/raw/test.csv', index=False)\n",
    "\n",
    "# Verify download\n",
    "train_df = pd.read_csv('data/raw/train.csv')\n",
    "test_df = pd.read_csv('data/raw/test.csv')\n",
    "print(f\"‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8464fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 16146 | Val: 1795 | Test: 11005\n"
     ]
    }
   ],
   "source": [
    "# Create train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train into train/val (90/10)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Save processed splits\n",
    "DATA_DIR = \"data/processed\"\n",
    "train_data.to_csv('data/processed/train.csv', index=False)\n",
    "val_data.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245029a",
   "metadata": {},
   "source": [
    "## 6. Run Experiments\n",
    "\n",
    "Configure and run your training experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b1566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint dir: /content/checkpoints\n",
      "Results dir: /content/results\n",
      "Plots dir: /content/plots\n"
     ]
    }
   ],
   "source": [
    "# Paths for persistent storage on Google Drive\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/checkpoints\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/results\"\n",
    "PLOTS_DIR = \"/content/drive/MyDrive/ML_Sentiment_Analysis/plots\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"Plots dir: {PLOTS_DIR}\")\n",
    "\n",
    "# ==================== EMBEDDINGS CONFIG ====================\n",
    "# Download Romanian fastText embeddings (run once - ~4.5GB)\n",
    "# !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ro.300.bin.gz\n",
    "# !gunzip cc.ro.300.bin.gz\n",
    "# !mv cc.ro.300.bin /content/drive/MyDrive/ML_Sentiment_Analysis/\n",
    "\n",
    "FASTTEXT_PATH = \"/content/drive/MyDrive/ML_Sentiment_Analysis/cc.ro.300.bin\"\n",
    "FREEZE_EMBEDDINGS = False\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741b4cf",
   "metadata": {},
   "source": [
    "### Experiment 1: LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a555918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python -m src.run_experiment --model lstm --hidden_dim 256 --num_layers 2 --epochs 20 --batch_size 64 --lr 0.001 --device cuda --experiment_name lstm_baseline --checkpoint_dir /content/checkpoints --results_dir /content/results --plots_dir /content/plots --pretrained_embeddings /content/cc.ro.300.bin\n",
      "Working directory: /content/sentiment_analysis\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-178809040.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Run and capture output to see errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python -m src.run_experiment \\\n",
    "    --model lstm \\\n",
    "    --hidden_dim 256 \\\n",
    "    --num_layers 2 \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.001 \\\n",
    "    --device {DEVICE} \\\n",
    "    --experiment_name lstm_baseline \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --plots_dir {PLOTS_DIR} \\\n",
    "    --pretrained_embeddings {FASTTEXT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c5f59",
   "metadata": {},
   "source": [
    "### Experiment 2: BiLSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python -m src.run_experiment --model bilstm_attention --hidden_dim 256 --num_layers 2 --epochs 20 --batch_size 64 --lr 0.001 --device cuda --experiment_name bilstm_attention --checkpoint_dir /content/checkpoints --results_dir /content/results --plots_dir /content/plots\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'src.run_experiment', '--model', 'bilstm_attention', '--hidden_dim', '256', '--num_layers', '2', '--epochs', '20', '--batch_size', '64', '--lr', '0.001', '--device', 'cuda', '--experiment_name', 'bilstm_attention', '--checkpoint_dir', '/content/checkpoints', '--results_dir', '/content/results', '--plots_dir', '/content/plots'], returncode=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m src.run_experiment \\\n",
    "    --model bilstm_attention \\\n",
    "    --hidden_dim 256 \\\n",
    "    --num_layers 2 \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.001 \\\n",
    "    --device {DEVICE} \\\n",
    "    --experiment_name bilstm_attention \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --plots_dir {PLOTS_DIR} \\\n",
    "    --pretrained_embeddings {FASTTEXT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655552e",
   "metadata": {},
   "source": [
    "### Experiment 3: LSTM with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e79436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.run_experiment \\\n",
    "    --model lstm \\\n",
    "    --hidden_dim 256 \\\n",
    "    --num_layers 2 \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.001 \\\n",
    "    --augment random_swap \\\n",
    "    --aug_prob 0.1 \\\n",
    "    --device {DEVICE} \\\n",
    "    --experiment_name lstm_augmented \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --plots_dir {PLOTS_DIR} \\\n",
    "    --pretrained_embeddings {FASTTEXT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91880724",
   "metadata": {},
   "source": [
    "Simple RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.run_experiment \\\n",
    "    --model simple_rnn \\\n",
    "    --embedding_dim 300 \\\n",
    "    --hidden_dim 256 \\\n",
    "    --num_layers 2 \\\n",
    "    --dropout 0.3 \\\n",
    "    --pooling last \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 1e-3 \\\n",
    "    --weight_decay 1e-5 \\\n",
    "    --optimizer adamw \\\n",
    "    --scheduler plateau \\\n",
    "    --gradient_clip 1.0 \\\n",
    "    --max_seq_len 128 \\\n",
    "    --min_freq 2 \\\n",
    "    --max_vocab_size 50000 \\\n",
    "    --augment none \\\n",
    "    --pretrained_embeddings {FASTTEXT_PATH} \\\n",
    "    --early_stopping 5 \\\n",
    "    --checkpoint_metric val_f1 \\\n",
    "    --experiment_name custom_experiment \\\n",
    "    --device {DEVICE} \\\n",
    "    --data_dir {DATA_DIR} \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --plots_dir {PLOTS_DIR} \\\n",
    "    --results_dir {RESULTS_DIR}\n",
    "\n",
    "    #--bidirectional \\\n",
    "    #--freeze_embeddings \\\n",
    "    #--resume {PATH}\n",
    "    #--evaluate_only /\n",
    "    #--checkpoint {PATH} \\\n",
    "    #--verbose \\\n",
    "    #--no_plots \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3011cb",
   "metadata": {},
   "source": [
    "Simple RNN aug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.run_experiment \\\n",
    "    --model simple_rnn \\\n",
    "    --embedding_dim 300 \\\n",
    "    --hidden_dim 256 \\\n",
    "    --num_layers 2 \\\n",
    "    --dropout 0.3 \\\n",
    "    --pooling last \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 1e-3 \\\n",
    "    --weight_decay 1e-5 \\\n",
    "    --optimizer adamw \\\n",
    "    --scheduler plateau \\\n",
    "    --gradient_clip 1.0 \\\n",
    "    --max_seq_len 128 \\\n",
    "    --min_freq 2 \\\n",
    "    --max_vocab_size 50000 \\\n",
    "    --augment eda \\\n",
    "    --aug_prob 0.1 \\\n",
    "    --aug_mode one_of \\\n",
    "    --pretrained_embeddings {FASTTEXT_PATH} \\\n",
    "    --early_stopping 5 \\\n",
    "    --checkpoint_metric val_f1 \\\n",
    "    --experiment_name simple_rnn_aug_eda \\\n",
    "    --device {DEVICE} \\\n",
    "    --data_dir {DATA_DIR} \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --plots_dir {PLOTS_DIR} \\\n",
    "    --results_dir {RESULTS_DIR}\n",
    "\n",
    "    #--bidirectional \\\n",
    "    #--freeze_embeddings \\\n",
    "    #--resume {PATH}\n",
    "    #--evaluate_only /\n",
    "    #--checkpoint {PATH} \\\n",
    "    #--verbose \\\n",
    "    #--no_plots \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aa1ea",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all experiment summaries\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "summaries = []\n",
    "\n",
    "if results_dir.exists():\n",
    "    for exp_dir in results_dir.iterdir():\n",
    "        if exp_dir.is_dir():\n",
    "            summary_file = exp_dir / 'summary.json'\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file) as f:\n",
    "                    summary = json.load(f)\n",
    "                    summary['experiment'] = exp_dir.name\n",
    "                    summaries.append(summary)\n",
    "\n",
    "if summaries:\n",
    "    df = pd.DataFrame(summaries)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    display(df[['experiment', 'model', 'best_val_f1', 'best_val_acc', 'epochs_trained']].sort_values('best_val_f1', ascending=False))\n",
    "else:\n",
    "    print(\"No results found yet. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2315c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots from experiments\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "plots_dir = Path(PLOTS_DIR)\n",
    "if plots_dir.exists():\n",
    "    for exp_dir in sorted(plots_dir.iterdir()):\n",
    "        if exp_dir.is_dir():\n",
    "            print(f\"\\nüìä {exp_dir.name}\")\n",
    "            print(\"-\" * 40)\n",
    "            for plot in sorted(exp_dir.glob('*.png')):\n",
    "                print(f\"\\n{plot.name}:\")\n",
    "                display(Image(filename=str(plot), width=600))\n",
    "else:\n",
    "    print(f\"Plots directory not found: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a8e6",
   "metadata": {},
   "source": [
    "## 8. Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ae040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download all results\n",
    "!zip -r /content/experiment_results.zip {RESULTS_DIR} {PLOTS_DIR}\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/experiment_results.zip')\n",
    "print(\"‚úÖ Results downloaded!\")\n",
    "\n",
    "# Note: Results are also saved to Google Drive at:\n",
    "print(f\"\\nüìÅ Results persist in Google Drive:\")\n",
    "print(f\"   {RESULTS_DIR}\")\n",
    "print(f\"   {PLOTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
