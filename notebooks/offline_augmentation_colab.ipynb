{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c7a2fd",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd91508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set project path in Drive\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/ML_Sentiment_Analysis\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q nlpaug transformers sentencepiece spacy pandas tqdm\n",
    "\n",
    "# Download spaCy Romanian model\n",
    "!python -m spacy download ro_core_news_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8086e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/setup project (if in Colab and not already present)\n",
    "import os\n",
    "\n",
    "# Option 1: Clone from GitHub (uncomment if using git)\n",
    "!git clone https://github.com/R3dP4ndaXD/sentiment_analysis.git\n",
    "%cd sentiment_analysis\n",
    "\n",
    "# Option 2: Use project from Drive\n",
    "# %cd {PROJECT_PATH}\n",
    "    \n",
    "# Verify project structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Verify imports work\n",
    "from src.data.augmentations import back_translate, contextual_word_replacement\n",
    "from src.preprocessing.text import tokenize, detokenize\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001926db",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119658f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Download ro_sent dataset\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/dumitrescustefan/Romanian-Transformers/examples/examples/sentiment_analysis/ro/test.csv\"\n",
    "\n",
    "!wget -q -O data/raw/train.csv \"{TRAIN_URL}\" 2>/dev/null || echo \"Downloading train.csv...\"\n",
    "!wget -q -O data/raw/test.csv \"{TEST_URL}\" 2>/dev/null || echo \"Downloading test.csv...\"\n",
    "\n",
    "# Check if download succeeded, if not try alternative method\n",
    "if not os.path.exists('data/raw/train.csv') or os.path.getsize('data/raw/train.csv') < 1000:\n",
    "    print(\"Trying alternative download method...\")\n",
    "    # Use datasets library as fallback\n",
    "    !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"dumitrescustefan/ro_sent\")\n",
    "    dataset['train'].to_pandas().to_csv('data/raw/train.csv', index=False)\n",
    "    dataset['test'].to_pandas().to_csv('data/raw/test.csv', index=False)\n",
    "\n",
    "# Verify download\n",
    "train_df = pd.read_csv('data/raw/train.csv')\n",
    "test_df = pd.read_csv('data/raw/test.csv')\n",
    "print(f\"✅ Train samples: {len(train_df)}\")\n",
    "print(f\"✅ Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train into train/val (85/15)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Save processed splits\n",
    "DATA_DIR = \"data/processed\"\n",
    "train_data.to_csv('data/processed/train.csv', index=False)\n",
    "val_data.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "print(f\"✅ Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed3a4b",
   "metadata": {},
   "source": [
    "## 2. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6433688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"data/processed/train.csv\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "\n",
    "# Class distribution\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in class_counts.items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  {sentiment} (label={label}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Calculate imbalance\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4871fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts from each class\n",
    "print(\"Sample NEGATIVE reviews:\")\n",
    "for text in train_df[train_df['label'] == 0]['text'].head(3):\n",
    "    print(f\"  • {text[:100]}...\")\n",
    "\n",
    "print(\"\\nSample POSITIVE reviews:\")\n",
    "for text in train_df[train_df['label'] == 1]['text'].head(3):\n",
    "    print(f\"  • {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393b63d",
   "metadata": {},
   "source": [
    "## 3. Test Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test back-translation (this will download M2M100 model ~2GB on first run)\n",
    "test_text = \"Produsul este foarte bun, recomand cu încredere!\"\n",
    "tokens = tokenize(test_text)\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# This may take a minute on first run (model download)\n",
    "print(\"\\nTesting back-translation (may take a minute on first run)...\")\n",
    "aug_tokens = back_translate(tokens, device=DEVICE)\n",
    "aug_text = detokenize(aug_tokens)\n",
    "print(f\"Back-translated: {aug_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eddb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test contextual word replacement\n",
    "print(f\"Original: {test_text}\")\n",
    "print(\"\\nTesting contextual word replacement...\")\n",
    "aug_tokens = contextual_word_replacement(tokens, n_replacements=2, device=DEVICE)\n",
    "aug_text = detokenize(aug_tokens)\n",
    "print(f\"Contextual replace: {aug_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55783ccb",
   "metadata": {},
   "source": [
    "## 4. Run Offline Augmentation\n",
    "\n",
    "Choose your augmentation strategy:\n",
    "\n",
    "| Strategy | Speed | Quality | Use Case |\n",
    "|----------|-------|---------|----------|\n",
    "| `back_translate` | Slow | High | Best paraphrasing |\n",
    "| `contextual_replace` | Medium | High | Semantic variation |\n",
    "| `synonym` | Fast | Medium | Simple word replacement |\n",
    "| `swap` + `delete` | Very Fast | Low | Structural noise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f215ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AUGMENTATION_CONFIG = {\n",
    "    \"input\": \"data/processed/train.csv\",\n",
    "    \"output\": \"data/augmented/train_balanced.csv\",\n",
    "    \"augment\": [\"back_translate\"],   # Options: back_translate, contextual_replace, contextual_insert\n",
    "    \"balance\": False,                 # Balance classes\n",
    "    \"augment_per_sample\": 0.15,\n",
    "    \"minority_only\": False,           # Only augment minority class\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"Augmentation Configuration:\")\n",
    "for k, v in AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build command\n",
    "cmd = f\"\"\"python -m src.data.offline_augmentation \\\n",
    "    --input {AUGMENTATION_CONFIG['input']} \\\n",
    "    --output {AUGMENTATION_CONFIG['output']} \\\n",
    "    --augment {' '.join(AUGMENTATION_CONFIG['augment'])} \\\n",
    "    --device {AUGMENTATION_CONFIG['device']} \\\n",
    "    --seed {AUGMENTATION_CONFIG['seed']}\"\"\"\n",
    "\n",
    "if AUGMENTATION_CONFIG['balance']:\n",
    "    cmd += f\" --balance --target_ratio {AUGMENTATION_CONFIG['target_ratio']}\"\n",
    "\n",
    "if AUGMENTATION_CONFIG.get('minority_only'):\n",
    "    cmd += \" --minority_only\"\n",
    "\n",
    "if AUGMENTATION_CONFIG.get('augment_per_sample'):\n",
    "    cmd += f\" --augment_per_sample {AUGMENTATION_CONFIG['augment_per_sample']}\"\n",
    "\n",
    "print(\"Command to run:\")\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run augmentation (this may take 30-60 minutes depending on dataset size and GPU)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75477d95",
   "metadata": {},
   "source": [
    "## 5. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented dataset\n",
    "output_path = AUGMENTATION_CONFIG['output']\n",
    "aug_df = pd.read_csv(output_path)\n",
    "\n",
    "print(f\"Augmented dataset: {len(aug_df)} samples\")\n",
    "print(f\"Original dataset: {len(train_df)} samples\")\n",
    "print(f\"Expansion: {len(aug_df) / len(train_df):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original\n",
    "orig_counts = train_df['label'].value_counts().sort_index()\n",
    "axes[0].bar(['Negative (0)', 'Positive (1)'], orig_counts.values, color=['#ff6b6b', '#51cf66'])\n",
    "axes[0].set_title('Original Dataset')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(orig_counts.values):\n",
    "    axes[0].text(i, v + 100, f'{v}\\n({v/len(train_df)*100:.1f}%)', ha='center')\n",
    "\n",
    "# Augmented\n",
    "aug_counts = aug_df['label'].value_counts().sort_index()\n",
    "axes[1].bar(['Negative (0)', 'Positive (1)'], aug_counts.values, color=['#ff6b6b', '#51cf66'])\n",
    "axes[1].set_title('Augmented Dataset')\n",
    "axes[1].set_ylabel('Count')\n",
    "for i, v in enumerate(aug_counts.values):\n",
    "    axes[1].text(i, v + 100, f'{v}\\n({v/len(aug_df)*100:.1f}%)', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/augmentation_balance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImbalance ratio:\")\n",
    "print(f\"  Original: {orig_counts.max()/orig_counts.min():.2f}:1\")\n",
    "print(f\"  Augmented: {aug_counts.max()/aug_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show augmented vs original breakdown\n",
    "if 'augmented' in aug_df.columns:\n",
    "    aug_breakdown = aug_df.groupby(['label', 'augmented']).size().unstack(fill_value=0)\n",
    "    print(\"Samples by type:\")\n",
    "    print(aug_breakdown)\n",
    "    \n",
    "    # Visualize\n",
    "    aug_breakdown.plot(kind='bar', stacked=True, figsize=(8, 5), \n",
    "                       color=['#74c0fc', '#ffd43b'])\n",
    "    plt.title('Original vs Augmented Samples by Class')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Original', 'Augmented'])\n",
    "    plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/augmentation_breakdown.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample augmented texts\n",
    "print(\"Sample AUGMENTED texts (minority class):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "minority_label = train_df['label'].value_counts().idxmin()\n",
    "augmented_samples = aug_df[(aug_df['label'] == minority_label) & (aug_df.get('augmented', False) == True)]\n",
    "\n",
    "for i, row in augmented_samples.head(5).iterrows():\n",
    "    print(f\"\\n[{row.get('aug_type', 'augmented')}]\")\n",
    "    print(f\"  {row['text'][:150]}...\" if len(row['text']) > 150 else f\"  {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bdba8",
   "metadata": {},
   "source": [
    "## 6. Copy to Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally, copy augmented data to Drive for later use\n",
    "\n",
    "import shutil\n",
    "\n",
    "drive_output = \"/content/drive/MyDrive/ML_Sentiment_Analysis/data/augmented/\"\n",
    "os.makedirs(drive_output, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy(output_path, drive_output)\n",
    "clean_path = output_path.replace('.csv', '_clean.csv')\n",
    "if os.path.exists(clean_path):\n",
    "    shutil.copy(clean_path, drive_output)\n",
    "\n",
    "print(f\"✓ Copied augmented data to {drive_output}\")\n",
    "!ls -la {drive_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44264b7",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Now you can train your model on the augmented dataset:\n",
    "\n",
    "```bash\n",
    "# Use the augmented data directory\n",
    "python -m src.run_experiment \\\n",
    "    --model bilstm \\\n",
    "    --data_dir data/augmented \\\n",
    "    --epochs 20 \\\n",
    "    --device cuda\n",
    "```\n",
    "\n",
    "**Note:** When using pre-augmented data:\n",
    "- DON'T use `--balance_classes` (already balanced)\n",
    "- DON'T use `--expand_factor` (already expanded)\n",
    "- You CAN still use light online augmentation (`--augment eda`) for variety"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
